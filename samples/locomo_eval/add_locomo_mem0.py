import json
import os
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from math import ceil
from threading import Lock

from dotenv import load_dotenv
from tqdm import tqdm

from mem0 import MemoryClient
from mem0.utils.comm import LLMClientFactory,load_json


load_dotenv()


# Update custom instructions
custom_instructions = """
Generate personal memories that follow these guidelines:

1. Each memory should be self-contained with complete context, including:
   - The person's name, do not use "user" while creating memories
   - Personal details (career aspirations, hobbies, life circumstances)
   - Emotional states and reactions
   - Ongoing journeys or future plans
   - Specific dates when events occurred

2. Include meaningful personal narratives focusing on:
   - Identity and self-acceptance journeys
   - Family planning and parenting
   - Creative outlets and hobbies
   - Mental health and self-care activities
   - Career aspirations and education goals
   - Important life events and milestones

3. Make each memory rich with specific details rather than general statements
   - Include timeframes (exact dates when possible)
   - Name specific activities (e.g., "charity race for mental health" rather than just "exercise")
   - Include emotional context and personal growth elements

4. Extract memories only from user messages, not incorporating assistant responses

5. Format each memory as a paragraph with a clear narrative structure that captures the person's experience, challenges, and aspirations
"""
# g_config = load_json(os.getenv("CONFIG_PATH", "config.json"))

HOST=os.getenv("DATABASE_MEM0_BASE_URL")

class MemoryADD:
    def __init__(self, data_path=None, batch_size=4, is_graph=False):
        print(f"{batch_size=},{is_graph=}")
        # benchmark_config = g_config["benchmark"]
        self.mem0_client = MemoryClient(host=HOST, api_key=os.getenv("DATABASE_MEM0_API_KEY"))
        self.batch_size = batch_size
        self.data_path = data_path
        self.data = None
        self.is_graph = is_graph
        if data_path:
            self.load_data()

    def load_data(self):
        with open(self.data_path, "r") as f:
            self.data = json.load(f)
        return self.data

    def add_memory(self, user_id, message, metadata, retries=3):
        for attempt in range(retries):
            try:
                _ = self.mem0_client.add(
                    message, user_id=user_id, metadata=metadata, enable_graph=self.is_graph
                )
                return
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(1)  # Wait before retrying
                    continue
                else:
                    raise e

    def add_memories_for_speaker(self, speaker, messages, timestamp, pbar, pbar_lock):
        for i in range(0, len(messages), self.batch_size):
            batch_messages = messages[i: i + self.batch_size]
            res = self.add_memory(speaker, batch_messages, metadata={"timestamp": timestamp})

            with pbar_lock:
                pbar.update(1)

    def process_conversation(self, item, idx, pbar, pbar_lock):
        conversation = item["conversation"]
        speaker_a = conversation["speaker_a"]
        speaker_b = conversation["speaker_b"]

        speaker_a_user_id = f"{speaker_a}_{idx}"
        speaker_b_user_id = f"{speaker_b}_{idx}"

        self.mem0_client.delete_all(user_id=speaker_a_user_id)
        self.mem0_client.delete_all(user_id=speaker_b_user_id)

        for key in conversation.keys():
            if key in ["speaker_a", "speaker_b"] or "date" in key or "timestamp" in key:
                continue
            date_time_key = key + "_date_time"
            timestamp = conversation[date_time_key]
            chats = conversation[key]

            messages = []
            messages_reverse = []
            for chat in chats:
                if chat["speaker"] == speaker_a:
                    messages.append({"role": "user", "content": f"{speaker_a}: {chat['text']}"})
                    messages_reverse.append({"role": "assistant", "content": f"{speaker_a}: {chat['text']}"})
                elif chat["speaker"] == speaker_b:
                    messages.append({"role": "assistant", "content": f"{speaker_b}: {chat['text']}"})
                    messages_reverse.append({"role": "user", "content": f"{speaker_b}: {chat['text']}"})
                else:
                    raise ValueError(f"Unknown speaker: {chat['speaker']}")

            thread_a = threading.Thread(
                target=self.add_memories_for_speaker,
                args=(speaker_a_user_id, messages, timestamp, pbar, pbar_lock),
            )
            thread_b = threading.Thread(
                target=self.add_memories_for_speaker,
                args=(speaker_b_user_id, messages_reverse, timestamp, pbar, pbar_lock),
            )

            thread_a.start()
            thread_b.start()
            thread_a.join()
            thread_b.join()

    def count_total_batches(self):
        total_batches = 0
        for item in self.data:
            conversation = item["conversation"]
            for key in conversation:
                if key in ["speaker_a", "speaker_b"] or "date" in key or "timestamp" in key:
                    continue
                chats = conversation[key]
                num_messages = len(chats)
                batches = ceil(num_messages / self.batch_size)
                total_batches += batches * 2  # speaker_a 和 speaker_b 各有一份
        return total_batches

    def process_all_conversations(self, max_workers=10):
        if not self.data:
            raise ValueError("No data loaded. Please set data_path and call load_data() first.")

        total_batches = self.count_total_batches()
        pbar = tqdm(total=total_batches, desc="Adding all memories")
        pbar_lock = Lock()

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for idx, item in enumerate(self.data):
                futures.append(
                    executor.submit(self.process_conversation, item, idx, pbar, pbar_lock)
                )
            for future in futures:
                future.result()
        pbar.close()
        
if __name__ == "__main__":
    memory_manager = MemoryADD(
        data_path=os.getenv("PATH_TO_LOCOMO_DATA"), 
        is_graph=False,
        batch_size=os.getenv("MEM0_BATCHSIZE", 4)
        )
    memory_manager.load_data()
    memory_manager.process_all_conversations(max_workers=1)
    # res = memory_manager.mem0_client.search('''Dave''', user_id='Calvin_9', output_format="v1.1", top_k=5)
    # print(res)