## Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http:##www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

#!/usr/bin/env python3
"""
ç®€æ´çš„æµ‹è¯•æ¡ˆä¾‹æŒ‡æ ‡æŸ¥çœ‹å™¨
å¿«é€ŸæŸ¥çœ‹æ¯ä¸ªtest caseçš„å„é¡¹æŒ‡æ ‡å¾—åˆ†å’ŒåŸå› 
"""

import json
import pandas as pd
from pathlib import Path
import argparse

def view_case_metrics(test_run_file: str, case_id: int = None, show_reasons: bool = False):
    """
    æŸ¥çœ‹æµ‹è¯•æ¡ˆä¾‹çš„æŒ‡æ ‡æ•°æ®
    
    Args:
        test_run_file: æµ‹è¯•è¿è¡ŒJSONæ–‡ä»¶è·¯å¾„
        case_id: ç‰¹å®šæ¡ˆä¾‹IDï¼ŒNoneè¡¨ç¤ºæŸ¥çœ‹æ‰€æœ‰
        show_reasons: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†åŸå› 
    """
    with open(test_run_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    test_cases = data['testRunData']['testCases']
    
    if case_id is not None:
        if case_id >= len(test_cases):
            print(f"âŒ æ¡ˆä¾‹ID {case_id} ä¸å­˜åœ¨ï¼Œæ€»å…±æœ‰ {len(test_cases)} ä¸ªæ¡ˆä¾‹")
            return
        test_cases = [test_cases[case_id]]
        start_id = case_id
    else:
        start_id = 0
    
    print("="*80)
    print("ğŸ“Š æµ‹è¯•æ¡ˆä¾‹æŒ‡æ ‡è¯¦æƒ…")
    print("="*80)
    
    for i, test_case in enumerate(test_cases):
        current_id = start_id + i if case_id is not None else i
        
        print(f"\nğŸ”¸ æ¡ˆä¾‹ {current_id}: {test_case.get('name', f'test_case_{current_id}')}")
        print(f"ğŸ“ è¾“å…¥: {test_case.get('input', '')[:100]}...")
        print(f"âœ… æ•´ä½“æˆåŠŸ: {test_case.get('success', False)}")
        print(f"â±ï¸  è¿è¡Œæ—¶é—´: {test_case.get('runDuration', 0):.2f}s")
        
        # æ˜¾ç¤ºå„ä¸ªæŒ‡æ ‡
        metrics_data = test_case.get('metricsData', [])
        
        if not metrics_data:
            print("   âš ï¸  æ— æŒ‡æ ‡æ•°æ®")
            continue
        
        print("\n   ğŸ“ˆ æŒ‡æ ‡è¯¦æƒ…:")
        print("   " + "-"*60)
        
        for metric in metrics_data:
            name = metric.get('name', 'Unknown')
            score = metric.get('score', 0)
            success = metric.get('success', False)
            threshold = metric.get('threshold', 0)
            
            # çŠ¶æ€å›¾æ ‡
            status_icon = "âœ…" if success else "âŒ"
            
            print(f"   {status_icon} {name}")
            print(f"      åˆ†æ•°: {score:.3f} (é˜ˆå€¼: {threshold})")
            
            if show_reasons:
                reason = metric.get('reason', '')
                if reason:
                    # æˆªæ–­è¿‡é•¿çš„åŸå› 
                    if len(reason) > 150:
                        reason = reason[:150] + "..."
                    print(f"      åŸå› : {reason}")
            
            print()

def create_metrics_table(test_run_file: str, output_file: str = None):
    """
    åˆ›å»ºæŒ‡æ ‡å¯¹æ¯”è¡¨æ ¼
    
    Args:
        test_run_file: æµ‹è¯•è¿è¡ŒJSONæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(test_run_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    test_cases = data['testRunData']['testCases']
    
    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    table_data = []
    
    for i, test_case in enumerate(test_cases):
        row = {
            'Case ID': i,
            'Case Name': test_case.get('name', f'test_case_{i}'),
            'Overall Success': 'âœ…' if test_case.get('success', False) else 'âŒ',
            'Duration (s)': f"{test_case.get('runDuration', 0):.2f}"
        }
        
        # æ·»åŠ å„ä¸ªæŒ‡æ ‡çš„åˆ†æ•°
        metrics_data = test_case.get('metricsData', [])
        for metric in metrics_data:
            name = metric.get('name', 'Unknown')
            score = metric.get('score', 0)
            success = metric.get('success', False)
            
            # ä½¿ç”¨ç®€åŒ–çš„åˆ—å
            col_name = name.replace(' ', '_').replace('-', '_')
            row[f'{col_name}_Score'] = f"{score:.3f}"
            row[f'{col_name}_Pass'] = 'âœ…' if success else 'âŒ'
        
        table_data.append(row)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(table_data)
    
    # æ˜¾ç¤ºè¡¨æ ¼
    print("\n" + "="*120)
    print("ğŸ“Š æŒ‡æ ‡å¯¹æ¯”è¡¨æ ¼")
    print("="*120)
    
    # è®¾ç½®æ˜¾ç¤ºé€‰é¡¹
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 15)
    
    print(df.to_string(index=False))
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"\nâœ… è¡¨æ ¼å·²ä¿å­˜åˆ°: {output_file}")

def show_metrics_statistics(test_run_file: str):
    """
    æ˜¾ç¤ºæŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        test_run_file: æµ‹è¯•è¿è¡ŒJSONæ–‡ä»¶è·¯å¾„
    """
    with open(test_run_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    test_cases = data['testRunData']['testCases']
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡æ•°æ®
    metrics_stats = {}
    
    for test_case in test_cases:
        metrics_data = test_case.get('metricsData', [])
        
        for metric in metrics_data:
            name = metric.get('name', 'Unknown')
            score = metric.get('score', 0)
            success = metric.get('success', False)
            
            if name not in metrics_stats:
                metrics_stats[name] = {
                    'scores': [],
                    'successes': [],
                    'total': 0
                }
            
            metrics_stats[name]['scores'].append(score)
            metrics_stats[name]['successes'].append(success)
            metrics_stats[name]['total'] += 1
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*80)
    print("ğŸ“ˆ æŒ‡æ ‡ç»Ÿè®¡æ±‡æ€»")
    print("="*80)
    
    for name, stats in metrics_stats.items():
        scores = stats['scores']
        successes = stats['successes']
        
        avg_score = sum(scores) / len(scores) if scores else 0
        success_rate = sum(successes) / len(successes) if successes else 0
        min_score = min(scores) if scores else 0
        max_score = max(scores) if scores else 0
        
        print(f"\nğŸ”¹ {name}")
        print(f"   å¹³å‡åˆ†æ•°: {avg_score:.3f}")
        print(f"   åˆ†æ•°èŒƒå›´: {min_score:.3f} - {max_score:.3f}")
        print(f"   é€šè¿‡ç‡: {success_rate:.1%}")
        print(f"   è¯„ä¼°æ¬¡æ•°: {stats['total']}")

def main():
    parser = argparse.ArgumentParser(description='æŸ¥çœ‹æµ‹è¯•æ¡ˆä¾‹çš„æŒ‡æ ‡æ•°æ®')
    parser.add_argument('--input', '-i', 
                       default='.deepeval/.latest_test_run.json',
                       help='è¾“å…¥çš„æµ‹è¯•è¿è¡ŒJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--case', '-c', type=int,
                       help='æŸ¥çœ‹ç‰¹å®šæ¡ˆä¾‹ID')
    parser.add_argument('--reasons', '-r', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†åŸå› ')
    parser.add_argument('--table', '-t', action='store_true',
                       help='æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼')
    parser.add_argument('--stats', '-s', action='store_true',
                       help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--output', '-o',
                       help='ä¿å­˜è¡¨æ ¼åˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.input).exists():
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
    
    try:
        if args.table:
            create_metrics_table(args.input, args.output)
        elif args.stats:
            show_metrics_statistics(args.input)
        else:
            view_case_metrics(args.input, args.case, args.reasons)
            
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()