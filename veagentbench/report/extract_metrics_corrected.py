#!/usr/bin/env python3
## Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http:##www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""
ä¿®æ­£ç‰ˆçš„Metricsæå–å·¥å…·
æ­£ç¡®æå–æ¯ä¸ªtest caseå¯¹åº”çš„metricsDataé‡Œé¢çš„scoreå’Œreasonç­‰å­—æ®µ
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any
import argparse

def extract_metrics_data(test_run_file: str) -> List[Dict[str, Any]]:
    """
    ä»æµ‹è¯•è¿è¡Œæ–‡ä»¶ä¸­æå–æ¯ä¸ªtest caseçš„metricsæ•°æ®
    
    Args:
        test_run_file: æµ‹è¯•è¿è¡ŒJSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«æ‰€æœ‰test case metricsæ•°æ®çš„åˆ—è¡¨
    """
    with open(test_run_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    test_cases = data['testRunData']['testCases']
    extracted_data = []
    
    for i, test_case in enumerate(test_cases):
        case_data = {
            'case_id': i,
            'case_name': test_case.get('name', f'test_case_{i}'),
            'input': test_case.get('input', ''),
            'actual_output': test_case.get('actualOutput', ''),
            'expected_output': test_case.get('expectedOutput', ''),
            'success': test_case.get('success', False),
            'run_duration': test_case.get('runDuration', 0)
        }
        
        # æå–metricsDataä¸­çš„æ¯ä¸ªæŒ‡æ ‡
        metrics_data = test_case.get('metricsData', [])
        
        for metric in metrics_data:
            metric_name = metric.get('name', 'Unknown')
            # æ¸…ç†æŒ‡æ ‡åç§°ï¼Œç”¨ä½œåˆ—å
            clean_name = metric_name.lower().replace(' ', '_').replace('-', '_')
            
            # æ·»åŠ æŒ‡æ ‡çš„å„ä¸ªå­—æ®µ
            case_data[f'{clean_name}_score'] = metric.get('score', None)
            case_data[f'{clean_name}_reason'] = metric.get('reason', '')
            case_data[f'{clean_name}_success'] = metric.get('success', False)
            case_data[f'{clean_name}_threshold'] = metric.get('threshold', None)
            case_data[f'{clean_name}_strict_mode'] = metric.get('strictMode', False)
            case_data[f'{clean_name}_evaluation_model'] = metric.get('evaluationModel', '')
            
            # æå–æ‹†è§£ç»´åº¦åˆ†æ•°ï¼ˆscore_breakdown / breakdown / dimension_scoresï¼‰
            breakdown = metric.get('score_breakdown')
            if breakdown is None:
                breakdown = metric.get('breakdown')
            if breakdown is None:
                breakdown = metric.get('dimension_scores')
            if isinstance(breakdown, dict):
                for dim_key, dim_val in breakdown.items():
                    dim_clean = str(dim_key).lower().replace(' ', '_').replace('-', '_')
                    case_data[f'{clean_name}_breakdown_{dim_clean}'] = dim_val
        
        # é¢„æœŸå·¥å…·è°ƒç”¨ï¼ˆexpected_tool_callsï¼‰ï¼Œå…¼å®¹ä¸¤ç§ä½ç½®
        expected_calls = test_case.get('expectedToolCalls')
        if expected_calls is None:
            execution_data = test_case.get('executionData') or {}
            expected_calls = execution_data.get('expectedToolCalls')
        # ä»¥JSONå­—ç¬¦ä¸²å½¢å¼ä¿å­˜åˆ°è¯¦ç»†è¡¨ï¼Œä¾¿äºæŸ¥çœ‹
        try:
            case_data['expected_tool_calls'] = json.dumps(expected_calls, ensure_ascii=False) if expected_calls is not None else ''
        except Exception:
            case_data['expected_tool_calls'] = str(expected_calls) if expected_calls is not None else ''
        
        extracted_data.append(case_data)
    
    return extracted_data

def create_metrics_summary(extracted_data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    åˆ›å»ºmetricsæ±‡æ€»ç»Ÿè®¡
    
    Args:
        extracted_data: æå–çš„metricsæ•°æ®
        
    Returns:
        æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
    """
    if not extracted_data:
        return {}
    
    # æ‰¾å‡ºæ‰€æœ‰çš„æŒ‡æ ‡åç§°
    metric_names = set()
    for case in extracted_data:
        for key in case.keys():
            if key.endswith('_score'):
                metric_name = key[:-6]  # ç§»é™¤'_score'åç¼€
                metric_names.add(metric_name)
    
    summary = {
        'total_cases': len(extracted_data),
        'overall_success_rate': sum(1 for case in extracted_data if case['success']) / len(extracted_data),
        'metrics_summary': {}
    }
    
    # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    for metric_name in metric_names:
        score_key = f'{metric_name}_score'
        success_key = f'{metric_name}_success'
        
        scores = [case[score_key] for case in extracted_data if case.get(score_key) is not None]
        successes = [case[success_key] for case in extracted_data if success_key in case]
        
        if scores:
            summary['metrics_summary'][metric_name] = {
                'avg_score': sum(scores) / len(scores),
                'min_score': min(scores),
                'max_score': max(scores),
                'success_rate': sum(successes) / len(successes) if successes else 0,
                'total_evaluations': len(scores)
            }
    
    return summary

def save_to_formats(extracted_data: List[Dict[str, Any]], summary: Dict[str, Any], output_prefix: str):
    """
    ä¿å­˜æ•°æ®åˆ°å¤šç§æ ¼å¼
    
    Args:
        extracted_data: æå–çš„æ•°æ®
        summary: æ±‡æ€»ç»Ÿè®¡
        output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
    """
    # ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(extracted_data)
    csv_file = f"{output_prefix}_detailed.csv"
    df.to_csv(csv_file, index=False, encoding='utf-8')
    print(f"âœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {csv_file}")
    
    # ä¿å­˜æ±‡æ€»ä¸ºJSON
    summary_file = f"{output_prefix}_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜åˆ°: {summary_file}")
    
    # åˆ›å»ºç®€åŒ–çš„metricsè¡¨æ ¼
    metrics_rows = []
    for case in extracted_data:
        row = {
            'case_id': case['case_id'],
            'case_name': case['case_name'],
            'overall_success': case['success']
        }
        
        # æ·»åŠ æ¯ä¸ªæŒ‡æ ‡çš„scoreä¸æ‹†è§£ç»´åº¦
        for key, value in case.items():
            if key.endswith('_score'):
                metric_name = key[:-6]
                row[f'{metric_name}_score'] = value
                row[f'{metric_name}_success'] = case.get(f'{metric_name}_success', False)
            # å°†æ‹†è§£ç»´åº¦åˆ†æ•°ä¹ŸåŠ å…¥ç®€åŒ–è¡¨
            if '_breakdown_' in key:
                row[key] = value
        
        metrics_rows.append(row)
    
    metrics_df = pd.DataFrame(metrics_rows)
    metrics_file = f"{output_prefix}_metrics_only.csv"
    metrics_df.to_csv(metrics_file, index=False, encoding='utf-8')
    print(f"âœ… æŒ‡æ ‡æ•°æ®å·²ä¿å­˜åˆ°: {metrics_file}")
    
    # ç”ŸæˆHTMLæŠ¥å‘Šï¼Œå‹å¥½å±•ç¤ºæ€»æŒ‡æ ‡ä¸æ‹†è§£ç»´åº¦
    try:
        html_file = f"{output_prefix}_report.html"
        
        # æ±‡æ€»è¡¨ï¼ˆmetrics_summaryï¼‰è½¬ä¸ºDataFrame
        metrics_summary = summary.get('metrics_summary', {})
        summary_rows = []
        for name, stats in metrics_summary.items():
            row = {'metric': name}
            row.update(stats)
            summary_rows.append(row)
        summary_df = pd.DataFrame(summary_rows)
        
        # HTMLå†…å®¹æ„å»º
        html_parts = []
        html_parts.append("<html><head><meta charset='utf-8'><title>Metrics Report</title>")
        html_parts.append("""
<style>
body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Microsoft Yahei", sans-serif; margin: 24px; }
h1, h2, h3 { color: #222; }
table { border-collapse: collapse; width: 100%; margin: 12px 0; }
th, td { border: 1px solid #ddd; padding: 8px; font-size: 13px; }
th { background: #f6f8fa; text-align: left; }
code, pre { background: #f6f8fa; padding: 8px; border-radius: 6px; }
.small { color: #666; font-size: 12px; }
.kpi { display: flex; gap: 16px; margin: 8px 0 16px; }
.kpi .card { background: #fafafa; border: 1px solid #eee; padding: 12px 16px; border-radius: 8px; }
.section { margin-top: 20px; }
</style>
</head><body>
""")
        html_parts.append("<h1>Metrics Report / æŒ‡æ ‡è¯„ä¼°æŠ¥å‘Š</h1>")
        html_parts.append(f"<div class='kpi'><div class='card'><b>Total cases</b><br>{summary.get('total_cases', 0)}</div>")
        overall_rate = summary.get('overall_success_rate', 0)
        html_parts.append(f"<div class='card'><b>Overall success rate</b><br>{overall_rate:.2%}</div></div>")
        
        # æ±‡æ€»è¡¨
        html_parts.append("<div class='section'><h2>Metrics Summary / æŒ‡æ ‡æ±‡æ€»</h2>")
        if not summary_df.empty:
            html_parts.append(summary_df.to_html(index=False))
        else:
            html_parts.append("<div class='small'>No metrics summary.</div>")
        html_parts.append("</div>")
        
        # ç®€åŒ–æŒ‡æ ‡è¡¨ï¼ˆå«æ‹†è§£ç»´åº¦ï¼‰
        html_parts.append("<div class='section'><h2>Metrics (Scores & Breakdowns) / æŒ‡æ ‡åˆ†æ•°ä¸æ‹†è§£ç»´åº¦</h2>")
        html_parts.append(metrics_df.to_html(index=False))
        html_parts.append("</div>")
        
        # è¯¦ç»†ç”¨ä¾‹è¡¨ï¼ˆå« expected_tool_calls é¢„è§ˆï¼‰
        html_parts.append("<div class='section'><h2>Detailed Cases / è¯¦ç»†ç”¨ä¾‹</h2>")
        detail_df = df.copy()
        def _shorten(x):
            try:
                s = str(x)
                return s if len(s) <= 300 else (s[:300] + "...(truncated)")
            except Exception:
                return x
        if 'expected_tool_calls' in detail_df.columns:
            detail_df['expected_tool_calls_preview'] = detail_df['expected_tool_calls'].apply(_shorten)
        html_parts.append(detail_df.to_html(index=False))
        html_parts.append("</div>")
        
        html_parts.append("<hr><div class='small'>Generated by extract_metrics_corrected.py</div></body></html>")
        
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write("".join(html_parts))
        print(f"âœ… HTML æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")
    except Exception as e:
        print(f"âš ï¸ ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {e}")

def print_metrics_overview(summary: Dict[str, Any]):
    """
    æ‰“å°metricsæ¦‚è§ˆ
    
    Args:
        summary: æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
    """
    print("\n" + "="*60)
    print("ğŸ“Š METRICS æ•°æ®æ¦‚è§ˆ")
    print("="*60)
    
    print(f"ğŸ“‹ æ€»æµ‹è¯•æ¡ˆä¾‹æ•°: {summary['total_cases']}")
    print(f"âœ… æ•´ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.2%}")
    
    print("\nğŸ“ˆ å„æŒ‡æ ‡ç»Ÿè®¡:")
    print("-" * 60)
    
    for metric_name, stats in summary['metrics_summary'].items():
        print(f"\nğŸ”¹ {metric_name.replace('_', ' ').title()}")
        print(f"   å¹³å‡åˆ†æ•°: {stats['avg_score']:.3f}")
        print(f"   åˆ†æ•°èŒƒå›´: {stats['min_score']:.3f} - {stats['max_score']:.3f}")
        print(f"   æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"   è¯„ä¼°æ¬¡æ•°: {stats['total_evaluations']}")

def main():
    parser = argparse.ArgumentParser(description='æå–æµ‹è¯•æ¡ˆä¾‹çš„metricsæ•°æ®')
    parser.add_argument('--input', '-i', 
                       default='.deepeval/.latest_test_run.json',
                       help='è¾“å…¥çš„æµ‹è¯•è¿è¡ŒJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o',
                       default='metrics_analysis',
                       help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    parser.add_argument('--show-details', '-d',
                       action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†çš„æ¡ˆä¾‹ä¿¡æ¯')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.input).exists():
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
    
    print(f"ğŸ” æ­£åœ¨åˆ†ææµ‹è¯•æ–‡ä»¶: {args.input}")
    
    # æå–æ•°æ®
    try:
        extracted_data = extract_metrics_data(args.input)
        summary = create_metrics_summary(extracted_data)
        
        # æ‰“å°æ¦‚è§ˆ
        print_metrics_overview(summary)
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if args.show_details and extracted_data:
            print("\n" + "="*60)
            print("ğŸ“‹ è¯¦ç»†æ¡ˆä¾‹ä¿¡æ¯ (å‰5ä¸ª)")
            print("="*60)
            
            for i, case in enumerate(extracted_data[:5]):
                print(f"\nğŸ”¸ æ¡ˆä¾‹ {case['case_id']}: {case['case_name']}")
                print(f"   è¾“å…¥: {case['input'][:100]}...")
                print(f"   æ•´ä½“æˆåŠŸ: {case['success']}")
                
                # æ˜¾ç¤ºå„ä¸ªæŒ‡æ ‡
                for key, value in case.items():
                    if key.endswith('_score'):
                        metric_name = key[:-6]
                        score = value
                        success = case.get(f'{metric_name}_success', False)
                        reason = case.get(f'{metric_name}_reason', '')[:100]
                        print(f"   ğŸ“Š {metric_name}: åˆ†æ•°={score}, æˆåŠŸ={success}")
                        if reason:
                            print(f"      åŸå› : {reason}...")
                        # å±•ç¤ºè¯¥æŒ‡æ ‡çš„æ‹†è§£ç»´åº¦åˆ†æ•°
                        breakdown_items = [(bk, bv) for bk, bv in case.items() if bk.startswith(f'{metric_name}_breakdown_')]
                        if breakdown_items:
                            print(f"      æ‹†è§£ç»´åº¦åˆ†æ•°:")
                            for bk, bv in sorted(breakdown_items):
                                dim = bk.replace(f'{metric_name}_breakdown_', '')
                                print(f"        - {dim}: {bv}")
                # å±•ç¤ºé¢„æœŸå·¥å…·è°ƒç”¨ï¼ˆæ¯ä¸ªç”¨ä¾‹ï¼‰
                if case.get('expected_tool_calls'):
                    preview = case['expected_tool_calls'][:200] + ("..." if len(case['expected_tool_calls']) > 200 else "")
                    print(f"   ğŸ”§ Expected tool calls: {preview}")
        
        # ä¿å­˜ç»“æœ
        save_to_formats(extracted_data, summary, args.output)
        
        print(f"\nâœ… åˆ†æå®Œæˆ! å…±å¤„ç† {len(extracted_data)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()