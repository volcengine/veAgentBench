#!/usr/bin/env python3
## Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http:##www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""
ä¿®æ­£ç‰ˆçš„Metricsæå–å·¥å…·
æ­£ç¡®æå–æ¯ä¸ªtest caseå¯¹åº”çš„metricsDataé‡Œé¢çš„scoreå’Œreasonç­‰å­—æ®µ
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any
import argparse
import ast



def safe_parse_json_string(json_str: str) -> Any:
    """
    å®‰å…¨åœ°è§£æå¯èƒ½åŒ…å«Pythonå­—é¢é‡çš„JSONå­—ç¬¦ä¸²
    """
    try:
        # é¦–å…ˆå°è¯•æ ‡å‡†JSONè§£æ
        return json.loads(json_str)
    except json.JSONDecodeError:
        try:
            # æ›¿æ¢Pythonå­—é¢é‡
            json_str = json_str.replace("true", "True").replace("false", "False").replace("null", "None")
            # ä½¿ç”¨ast.literal_eval
            return ast.literal_eval(json_str)
        except (ValueError, SyntaxError):
            # æœ€åå°è¯•eval
            return eval(json_str)

def extract_metrics_data(test_run_file: str) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
    """
    ä»æµ‹è¯•è¿è¡Œæ–‡ä»¶ä¸­æå–æ¯ä¸ªtest caseçš„metricsæ•°æ®
    
    Args:
        test_run_file: æµ‹è¯•è¿è¡ŒJSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«æ‰€æœ‰taskå’Œdatasetçš„metricsæ•°æ®çš„å­—å…¸
        æ ¼å¼: {task_name: {dataset_name: [test_cases...]}}
    """
    
    with open(test_run_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    task_testcases_all = _extract_metrics_data(data)
    return task_testcases_all

def _extract_metrics_data(data):
    task_testcases_all = {}

    # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šç›´æ¥æ•°ç»„æˆ–åŒ…å«testRunDataçš„å¯¹è±¡
    if isinstance(data, list):
        # ç›´æ¥æ˜¯ä»»åŠ¡ç»“æœæ•°ç»„
        for task_result in data:
            task_name = task_result['task_name']
            task_testcases_all[task_name] = {}
            
            if 'result' in task_result and isinstance(task_result['result'], dict):
                measure_result = task_result['result'].get('result', '')
                if measure_result and isinstance(measure_result, str):
                    try:
                        # è§£æmeasure_resultå­—ç¬¦ä¸²
                        measure_data = safe_parse_json_string(measure_result)
                        
                        if isinstance(measure_data, list) and len(measure_data) > 0:
                            # measure_dataæ˜¯æ•°ç»„ï¼Œå¤„ç†æ¯ä¸ªdatasetçš„æ•°æ®
                            for dataset_item in measure_data:
                                dataset_name = dataset_item.get("dataset_name", "default")
                                task_testcases_all[task_name].setdefault(
                                    dataset_name, [])
                                
                                if isinstance(dataset_item, dict) and 'measure_result' in dataset_item:
                                    test_results = dataset_item.get('measure_result', '')
                                    if test_results and isinstance(test_results, str):
                                        test_data = safe_parse_json_string(test_results)
                                        
                                        if 'test_results' in test_data:
                                            extracted_cases = extract_metrics_data_from_testcases(test_data['test_results'])
                                            # ä¸ºæ¯ä¸ªcaseæ·»åŠ dataset_nameä¿¡æ¯
                                            for case in extracted_cases:
                                                case['dataset_name'] = dataset_name
                                                case['task_name'] = task_name
                                            task_testcases_all[task_name][dataset_name].extend(extracted_cases)
                                
                    except Exception as e:
                        print(f"âš ï¸ è§£æä»»åŠ¡ç»“æœå¤±è´¥: {e}")
                        continue
    
    return task_testcases_all


def extract_metrics_data_from_testcases(test_cases: List[Dict[str, Any]]):
    
    extracted_data = []
    
    for i, test_case in enumerate(test_cases):
        case_data = {
            'case_id': i,
            'case_name': test_case.get('name', f'test_case_{i}'),
            'input': test_case.get('input', ''),
            'actual_output': test_case.get('actual_output', ''),
            'expected_output': test_case.get('expected_output', ''),
            'success': test_case.get('success', False),
            'run_duration': test_case.get('runDuration', 0)
        }
        
        # æå–metrics_dataä¸­çš„æ¯ä¸ªæŒ‡æ ‡ï¼ˆæ”¯æŒå¤šç§é”®åæ ¼å¼ï¼‰
        metrics_data = test_case.get('metrics_data') or test_case.get('metricsData', [])
        
        for metric in metrics_data:
            metric_name = metric.get('name', 'Unknown')
            # æ¸…ç†æŒ‡æ ‡åç§°ï¼Œç”¨ä½œåˆ—å
            clean_name = metric_name.lower().replace(' ', '_').replace('-', '_')
            
            # æ·»åŠ æŒ‡æ ‡çš„å„ä¸ªå­—æ®µ
            case_data[f'{clean_name}_score'] = metric.get('score', None)
            case_data[f'{clean_name}_reason'] = metric.get('reason', '')
            case_data[f'{clean_name}_success'] = metric.get('success', False)
            case_data[f'{clean_name}_threshold'] = metric.get('threshold', None)
            case_data[f'{clean_name}_strict_mode'] = metric.get('strictMode', False)
            case_data[f'{clean_name}_evaluation_model'] = metric.get('evaluationModel', '')
            
            # æå–æ‹†è§£ç»´åº¦åˆ†æ•°ï¼ˆscore_breakdown / breakdown / dimension_scoresï¼‰
            breakdown = metric.get('score_breakdown')
            if breakdown is None:
                breakdown = metric.get('breakdown')
            if breakdown is None:
                breakdown = metric.get('dimension_scores')
            if isinstance(breakdown, dict):
                for dim_key, dim_val in breakdown.items():
                    dim_clean = str(dim_key).lower().replace(' ', '_').replace('-', '_')
                    case_data[f'{clean_name}_breakdown_{dim_clean}'] = dim_val
        
        # é¢„æœŸå·¥å…·è°ƒç”¨ï¼ˆexpected_tool_callsï¼‰ï¼Œå…¼å®¹å¤šç§é”®åæ ¼å¼
        expected_calls = test_case.get('expected_tool_calls') or test_case.get('expectedToolCalls')
        if expected_calls is None:
            execution_data = test_case.get('execution_data') or test_case.get('executionData') or {}
            expected_calls = execution_data.get('expected_tool_calls') or execution_data.get('expectedToolCalls')
        # ä»¥JSONå­—ç¬¦ä¸²å½¢å¼ä¿å­˜åˆ°è¯¦ç»†è¡¨ï¼Œä¾¿äºæŸ¥çœ‹
        try:
            case_data['expected_tool_calls'] = json.dumps(expected_calls, ensure_ascii=False) if expected_calls is not None else ''
        except Exception:
            case_data['expected_tool_calls'] = str(expected_calls) if expected_calls is not None else ''
        
        extracted_data.append(case_data)
    
    return extracted_data

def create_metrics_summary(extracted_data: List[Dict[str, Any]], group_by_dataset: bool = False) -> Dict[str, Any]:
    """
    åˆ›å»ºmetricsæ±‡æ€»ç»Ÿè®¡ï¼ŒåŒ…å«breakdownç»´åº¦æŒ‡æ ‡çš„æ±‡æ€»
    
    Args:
        extracted_data: æå–çš„metricsæ•°æ®
        group_by_dataset: æ˜¯å¦æŒ‰datasetåˆ†ç»„ç»Ÿè®¡
        
    Returns:
        æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
    """
    if not extracted_data:
        return {}
    
    # æ‰¾å‡ºæ‰€æœ‰çš„æŒ‡æ ‡åç§°å’Œå¯¹åº”çš„breakdownç»´åº¦
    metric_names = set()
    breakdown_dimensions = {}  # metric_name -> set of breakdown dimensions
    
    for case in extracted_data:
        for key in case.keys():
            if key.endswith('_score'):
                metric_name = key[:-6]  # ç§»é™¤'_score'åç¼€
                metric_names.add(metric_name)
                
                # æ”¶é›†è¯¥æŒ‡æ ‡çš„breakdownç»´åº¦
                breakdown_prefix = f'{metric_name}_breakdown_'
                for case_key in case.keys():
                    if case_key.startswith(breakdown_prefix) and case.get(case_key) is not None:
                        dim_name = case_key[len(breakdown_prefix):]
                        if metric_name not in breakdown_dimensions:
                            breakdown_dimensions[metric_name] = set()
                        breakdown_dimensions[metric_name].add(dim_name)
    
    def calculate_breakdown_stats(cases, metric_name):
        """è®¡ç®—æŒ‡å®šæŒ‡æ ‡çš„breakdownç»´åº¦ç»Ÿè®¡"""
        breakdown_stats = {}
        if metric_name in breakdown_dimensions:
            for dim_name in breakdown_dimensions[metric_name]:
                dim_key = f'{metric_name}_breakdown_{dim_name}'
                dim_values = [case[dim_key] for case in cases if case.get(dim_key) is not None]
                if dim_values:
                    breakdown_stats[dim_name] = {
                        'avg_score': sum(dim_values) / len(dim_values),
                        'min_score': min(dim_values),
                        'max_score': max(dim_values),
                        'total_evaluations': len(dim_values)
                    }
        return breakdown_stats
    
    if group_by_dataset and extracted_data and 'dataset_name' in extracted_data[0]:
        # æŒ‰datasetåˆ†ç»„ç»Ÿè®¡
        datasets = {}
        for case in extracted_data:
            dataset_name = case['dataset_name']
            if dataset_name not in datasets:
                datasets[dataset_name] = []
            datasets[dataset_name].append(case)
        
        summary = {
            'total_cases': len(extracted_data),
            'overall_success_rate': sum(1 for case in extracted_data if case['success']) / len(extracted_data),
            'datasets_summary': {},
            'metrics_summary': {}
        }
        
        # ä¸ºæ¯ä¸ªdatasetåˆ›å»ºç»Ÿè®¡
        for dataset_name, dataset_cases in datasets.items():
            dataset_summary = {
                'total_cases': len(dataset_cases),
                'overall_success_rate': sum(1 for case in dataset_cases if case['success']) / len(dataset_cases),
                'metrics_summary': {}
            }
            
            # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            for metric_name in metric_names:
                score_key = f'{metric_name}_score'
                success_key = f'{metric_name}_success'
                
                scores = [case[score_key] for case in dataset_cases if case.get(score_key) is not None]
                successes = [case[success_key] for case in dataset_cases if success_key in case]
                
                if scores:
                    metric_summary = {
                        'avg_score': sum(scores) / len(scores),
                        'min_score': min(scores),
                        'max_score': max(scores),
                        'success_rate': sum(successes) / len(successes) if successes else 0,
                        'total_evaluations': len(scores)
                    }
                    
                    # æ·»åŠ breakdownç»´åº¦ç»Ÿè®¡
                    breakdown_stats = calculate_breakdown_stats(dataset_cases, metric_name)
                    if breakdown_stats:
                        metric_summary['breakdown_summary'] = breakdown_stats
                    
                    dataset_summary['metrics_summary'][metric_name] = metric_summary
            
            summary['datasets_summary'][dataset_name] = dataset_summary
        
        # è®¡ç®—æ•´ä½“çš„metrics_summary
        for metric_name in metric_names:
            score_key = f'{metric_name}_score'
            success_key = f'{metric_name}_success'
            
            scores = [case[score_key] for case in extracted_data if case.get(score_key) is not None and case[score_key] is not -1]
            successes = [case[success_key] for case in extracted_data if success_key in case]
            
            if scores:
                metric_summary = {
                    'avg_score': sum(scores) / len(scores),
                    'min_score': min(scores),
                    'max_score': max(scores),
                    'success_rate': sum(successes) / len(successes) if successes else 0,
                    'total_evaluations': len(scores)
                }
                
                # æ·»åŠ æ•´ä½“çš„breakdownç»´åº¦ç»Ÿè®¡
                breakdown_stats = calculate_breakdown_stats(extracted_data, metric_name)
                if breakdown_stats:
                    metric_summary['breakdown_summary'] = breakdown_stats
                
                summary['metrics_summary'][metric_name] = metric_summary
        
        return summary
    else:
        # åŸå§‹çš„å•datasetç»Ÿè®¡é€»è¾‘
        summary = {
            'total_cases': len(extracted_data),
            'overall_success_rate': sum(1 for case in extracted_data if case['success']) / len(extracted_data),
            'metrics_summary': {}
        }
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        for metric_name in metric_names:
            score_key = f'{metric_name}_score'
            success_key = f'{metric_name}_success'
            
            scores = [case[score_key] for case in extracted_data if case.get(score_key) is not None and case[score_key] is not -1]
            successes = [case[success_key] for case in extracted_data if success_key in case]
            
            if scores:
                metric_summary = {
                    'avg_score': sum(scores) / len(scores),
                    'min_score': min(scores),
                    'max_score': max(scores),
                    'success_rate': sum(successes) / len(successes) if successes else 0,
                    'total_evaluations': len(scores)
                }
                
                # æ·»åŠ breakdownç»´åº¦ç»Ÿè®¡
                breakdown_stats = calculate_breakdown_stats(extracted_data, metric_name)
                if breakdown_stats:
                    metric_summary['breakdown_summary'] = breakdown_stats
                
                summary['metrics_summary'][metric_name] = metric_summary
        
        return summary

def save_to_formats(extracted_data: List[Dict[str, Any]], summary: Dict[str, Any], output_prefix: str, multi_dataset: bool = False):
    """
    ä¿å­˜æ•°æ®åˆ°å¤šç§æ ¼å¼
    
    Args:
        extracted_data: æå–çš„æ•°æ®
        summary: æ±‡æ€»ç»Ÿè®¡
        output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
        multi_dataset: æ˜¯å¦ä¸ºå¤šdatasetæ¨¡å¼
    """
    # ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(extracted_data)
    csv_file = f"{output_prefix}_detailed.csv"
    df.to_csv(csv_file, index=False, encoding='utf-8')
    print(f"âœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {csv_file}")
    
    # ä¿å­˜æ±‡æ€»ä¸ºJSON
    summary_file = f"{output_prefix}_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜åˆ°: {summary_file}")
    
    # åˆ›å»ºç®€åŒ–çš„metricsè¡¨æ ¼
    metrics_rows = []
    for case in extracted_data:
        row = {
            'case_id': case['case_id'],
            'case_name': case['case_name'],
            'overall_success': case['success']
        }
        
        # æ·»åŠ datasetåç§°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'dataset_name' in case:
            row['dataset_name'] = case['dataset_name']
        
        # æ·»åŠ æ¯ä¸ªæŒ‡æ ‡çš„scoreä¸æ‹†è§£ç»´åº¦
        for key, value in case.items():
            if key.endswith('_score'):
                metric_name = key[:-6]
                row[f'{metric_name}_score'] = value
                row[f'{metric_name}_success'] = case.get(f'{metric_name}_success', False)
            # å°†æ‹†è§£ç»´åº¦åˆ†æ•°ä¹ŸåŠ å…¥ç®€åŒ–è¡¨
            if '_breakdown_' in key:
                row[key] = value
        
        metrics_rows.append(row)
    
    metrics_df = pd.DataFrame(metrics_rows)
    metrics_file = f"{output_prefix}_metrics_only.csv"
    metrics_df.to_csv(metrics_file, index=False, encoding='utf-8')
    print(f"âœ… æŒ‡æ ‡æ•°æ®å·²ä¿å­˜åˆ°: {metrics_file}")
    
    # ç”ŸæˆHTMLæŠ¥å‘Šï¼Œå‹å¥½å±•ç¤ºæ€»æŒ‡æ ‡ä¸æ‹†è§£ç»´åº¦
    try:
        html_file = f"{output_prefix}_report.html"
        
        # HTMLå†…å®¹æ„å»º
        html_parts = []
        html_parts.append("<html><head><meta charset='utf-8'><title>Metrics Report</title>")
        html_parts.append("""
<style>
body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Microsoft Yahei", sans-serif; margin: 24px; }
h1, h2, h3 { color: #222; }
table { border-collapse: collapse; width: 100%; margin: 12px 0; }
th, td { border: 1px solid #ddd; padding: 8px; font-size: 13px; }
th { background: #f6f8fa; text-align: left; }
code, pre { background: #f6f8fa; padding: 8px; border-radius: 6px; }
.small { color: #666; font-size: 12px; }
.kpi { display: flex; gap: 16px; margin: 8px 0 16px; }
.kpi .card { background: #fafafa; border: 1px solid #eee; padding: 12px 16px; border-radius: 8px; }
.section { margin-top: 20px; }
.dataset-section { background: #f9f9f9; padding: 16px; border-radius: 8px; margin: 16px 0; }
</style>
</head><body>
""")
        html_parts.append("<h1>Metrics Report / æŒ‡æ ‡è¯„ä¼°æŠ¥å‘Š</h1>")
        html_parts.append(f"<div class='kpi'><div class='card'><b>Total cases</b><br>{summary.get('total_cases', 0)}</div>")
        overall_rate = summary.get('overall_success_rate', 0)
        html_parts.append(f"<div class='card'><b>Overall success rate</b><br>{overall_rate:.2%}</div></div>")
        
        # å¤šdatasetæ¨¡å¼ï¼šåˆ†åˆ«æ˜¾ç¤ºæ¯ä¸ªdatasetçš„ç»Ÿè®¡
        if multi_dataset and 'datasets_summary' in summary:
            html_parts.append("<div class='section'><h2>Datasets Summary / æ•°æ®é›†æ±‡æ€»</h2>")
            
            # æ±‡æ€»è¡¨ï¼ˆæ‰€æœ‰datasetçš„æ¦‚è§ˆï¼‰
            datasets_overview = []
            for dataset_name, dataset_stats in summary['datasets_summary'].items():
                overview_row = {
                    'dataset_name': dataset_name,
                    'total_cases': dataset_stats['total_cases'],
                    'overall_success_rate': f"{dataset_stats['overall_success_rate']:.2%}",
                    'metrics_count': len(dataset_stats.get('metrics_summary', {}))
                }
                datasets_overview.append(overview_row)
            
            if datasets_overview:
                overview_df = pd.DataFrame(datasets_overview)
                html_parts.append("<h3>Dataset Overview / æ•°æ®é›†æ¦‚è§ˆ</h3>")
                html_parts.append(overview_df.to_html(index=False))
            
            # æ¯ä¸ªdatasetçš„è¯¦ç»†ç»Ÿè®¡
            for dataset_name, dataset_stats in summary['datasets_summary'].items():
                html_parts.append(f"<div class='dataset-section'>")
                html_parts.append(f"<h3>Dataset: {dataset_name}</h3>")
                html_parts.append(f"<div class='kpi'><div class='card'><b>Cases</b><br>{dataset_stats['total_cases']}</div>")
                html_parts.append(f"<div class='card'><b>Success Rate</b><br>{dataset_stats['overall_success_rate']:.2%}</div></div>")
                
                # datasetçš„metricsç»Ÿè®¡
                dataset_metrics = []
                for metric_name, metric_stats in dataset_stats.get('metrics_summary', {}).items():
                    metric_row = {'metric': metric_name}
                    metric_row.update(metric_stats)
                    dataset_metrics.append(metric_row)
                
                if dataset_metrics:
                    dataset_metrics_df = pd.DataFrame(dataset_metrics)
                    html_parts.append("<h4>Metrics Summary / æŒ‡æ ‡æ±‡æ€»</h4>")
                    html_parts.append(dataset_metrics_df.to_html(index=False))
                
                html_parts.append("</div>")
        
        # æ•´ä½“æ±‡æ€»è¡¨ï¼ˆmetrics_summaryï¼‰
        html_parts.append("<div class='section'><h2>Overall Metrics Summary / æ•´ä½“æŒ‡æ ‡æ±‡æ€»</h2>")
        metrics_summary = summary.get('metrics_summary', {})
        summary_rows = []
        for name, stats in metrics_summary.items():
            row = {'metric': name}
            row.update(stats)
            summary_rows.append(row)
        
        if summary_rows:
            summary_df = pd.DataFrame(summary_rows)
            html_parts.append(summary_df.to_html(index=False))
        else:
            html_parts.append("<div class='small'>No metrics summary.</div>")
        html_parts.append("</div>")
        
        # Breakdownç»´åº¦æ±‡æ€»è¡¨æ ¼
        if multi_dataset and 'datasets_summary' in summary:
            html_parts.append("<div class='section'><h2>Breakdown Dimensions Summary by Dataset / å„æ•°æ®é›†æ‹†è§£ç»´åº¦æ±‡æ€»</h2>")
            
            # æ”¶é›†æ‰€æœ‰breakdownç»´åº¦çš„æ•°æ®
            breakdown_data = []
            for dataset_name, dataset_stats in summary['datasets_summary'].items():
                for metric_name, metric_stats in dataset_stats.get('metrics_summary', {}).items():
                    if 'breakdown_summary' in metric_stats:
                        for dim_name, dim_stats in metric_stats['breakdown_summary'].items():
                            breakdown_data.append({
                                'dataset_name': dataset_name,
                                'metric_name': metric_name,
                                'dimension_name': dim_name,
                                'avg_score': dim_stats['avg_score'],
                                'min_score': dim_stats['min_score'],
                                'max_score': dim_stats['max_score'],
                                'total_evaluations': dim_stats['total_evaluations']
                            })
            
            if breakdown_data:
                breakdown_df = pd.DataFrame(breakdown_data)
                
                # åˆ›å»ºé€è§†è¡¨ï¼ŒæŒ‰æ•°æ®é›†å’Œç»´åº¦å±•ç¤º
                pivot_table = breakdown_df.pivot_table(
                    index=['metric_name', 'dimension_name'],
                    columns='dataset_name',
                    values='avg_score',
                    aggfunc='mean'
                ).round(3)
                
                html_parts.append("<h3>Average Scores by Dataset and Dimension / å„æ•°æ®é›†ç»´åº¦å¹³å‡åˆ†</h3>")
                html_parts.append(pivot_table.to_html())
                
                # æ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨
                html_parts.append("<h3>Detailed Breakdown Data / è¯¦ç»†æ‹†è§£æ•°æ®</h3>")
                html_parts.append(breakdown_df.to_html(index=False))
            else:
                html_parts.append("<div class='small'>No breakdown dimensions found.</div>")
            
            html_parts.append("</div>")
        
        # ç®€åŒ–æŒ‡æ ‡è¡¨ï¼ˆå«æ‹†è§£ç»´åº¦ï¼‰
        html_parts.append("<div class='section'><h2>Metrics (Scores & Breakdowns) / æŒ‡æ ‡åˆ†æ•°ä¸æ‹†è§£ç»´åº¦</h2>")
        html_parts.append(metrics_df.to_html(index=False))
        html_parts.append("</div>")
        
        # è¯¦ç»†ç”¨ä¾‹è¡¨ï¼ˆå« expected_tool_calls é¢„è§ˆï¼‰
        html_parts.append("<div class='section'><h2>Detailed Cases / è¯¦ç»†ç”¨ä¾‹</h2>")
        detail_df = df.copy()
        def _shorten(x):
            try:
                s = str(x)
                return s if len(s) <= 300 else (s[:300] + "...(truncated)")
            except Exception:
                return x
        if 'expected_tool_calls' in detail_df.columns:
            detail_df['expected_tool_calls_preview'] = detail_df['expected_tool_calls'].apply(_shorten)
        html_parts.append(detail_df.to_html(index=False))
        html_parts.append("</div>")
        
        html_parts.append("<hr><div class='small'>Generated by extract_metrics_corrected.py</div></body></html>")
        
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write("".join(html_parts))
        print(f"âœ… HTML æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")
    except Exception as e:
        print(f"âš ï¸ ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {e}")

def print_metrics_overview(summary: Dict[str, Any], multi_dataset: bool = False):
    """
    æ‰“å°metricsæ¦‚è§ˆï¼ŒåŒ…å«breakdownç»´åº¦ç»Ÿè®¡
    
    Args:
        summary: æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        multi_dataset: æ˜¯å¦ä¸ºå¤šdatasetæ¨¡å¼
    """
    print("\n" + "="*60)
    print("ğŸ“Š METRICS æ•°æ®æ¦‚è§ˆ")
    print("="*60)
    
    print(f"ğŸ“‹ æ€»æµ‹è¯•æ¡ˆä¾‹æ•°: {summary['total_cases']}")
    print(f"âœ… æ•´ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.2%}")
    
    # å¤šdatasetæ¨¡å¼ï¼šæ˜¾ç¤ºæ¯ä¸ªdatasetçš„æ¦‚è§ˆ
    if multi_dataset and 'datasets_summary' in summary:
        print("\nğŸ“Š å„æ•°æ®é›†ç»Ÿè®¡:")
        print("-" * 60)
        for dataset_name, dataset_stats in summary['datasets_summary'].items():
            print(f"\nğŸ“ Dataset: {dataset_name}")
            print(f"   æ¡ˆä¾‹æ•°: {dataset_stats['total_cases']}")
            print(f"   æˆåŠŸç‡: {dataset_stats['overall_success_rate']:.2%}")
    
    print("\nğŸ“ˆ å„æŒ‡æ ‡ç»Ÿè®¡:")
    print("-" * 60)
    
    for metric_name, stats in summary['metrics_summary'].items():
        print(f"\nğŸ”¹ {metric_name.replace('_', ' ').title()}")
        print(f"   å¹³å‡åˆ†æ•°: {stats['avg_score']:.3f}")
        print(f"   åˆ†æ•°èŒƒå›´: {stats['min_score']:.3f} - {stats['max_score']:.3f}")
        print(f"   æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"   è¯„ä¼°æ¬¡æ•°: {stats['total_evaluations']}")
        
        # æ˜¾ç¤ºbreakdownç»´åº¦ç»Ÿè®¡
        if 'breakdown_summary' in stats:
            print(f"   ğŸ“Š æ‹†è§£ç»´åº¦ç»Ÿè®¡:")
            for dim_name, dim_stats in stats['breakdown_summary'].items():
                print(f"     - {dim_name.replace('_', ' ').title()}:")
                print(f"       å¹³å‡åˆ†æ•°: {dim_stats['avg_score']:.3f}")
                print(f"       åˆ†æ•°èŒƒå›´: {dim_stats['min_score']:.3f} - {dim_stats['max_score']:.3f}")
                print(f"       è¯„ä¼°æ¬¡æ•°: {dim_stats['total_evaluations']}")

def process_single_task(task_name: str, task_data: Dict[str, List[Dict[str, Any]]], output_prefix: str, show_details: bool = False):
    """
    å¤„ç†å•ä¸ªtaskçš„æ•°æ®ï¼Œç”Ÿæˆç‹¬ç«‹çš„æŠ¥å‘Šæ–‡ä»¶
    
    Args:
        task_name: ä»»åŠ¡åç§°
        task_data: è¯¥ä»»åŠ¡çš„æ•°æ®ï¼Œæ ¼å¼ä¸º {dataset_name: [test_cases...]}
        output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    print(f"\nğŸ“Š å¤„ç†ä»»åŠ¡: {task_name}")
    
    # åˆå¹¶æ‰€æœ‰datasetçš„æµ‹è¯•æ¡ˆä¾‹
    all_task_cases = []
    total_cases = 0
    
    for dataset_name, cases in task_data.items():
        all_task_cases.extend(cases)
        total_cases += len(cases)
        print(f"   Dataset '{dataset_name}': {len(cases)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
    
    if not all_task_cases:
        print(f"   âš ï¸ ä»»åŠ¡ {task_name} æ²¡æœ‰æµ‹è¯•æ¡ˆä¾‹")
        return
    
    print(f"   æ€»è®¡: {total_cases} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
    
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¤šdatasetæ¨¡å¼ï¼ˆå•ä¸ªä»»åŠ¡å†…æœ‰å¤šä¸ªdatasetï¼‰
    multi_dataset = len(task_data) > 1
    
    # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡
    summary = create_metrics_summary(all_task_cases, group_by_dataset=multi_dataset)
    
    # æ‰“å°æ¦‚è§ˆ
    print_metrics_overview(summary, multi_dataset=multi_dataset)
    
    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    if show_details and all_task_cases:
        print("\n" + "="*60)
        print("ğŸ“‹ è¯¦ç»†æ¡ˆä¾‹ä¿¡æ¯ (å‰5ä¸ª)")
        print("="*60)
        
        for i, case in enumerate(all_task_cases[:5]):
            dataset_info = f" [{case['dataset_name']}]" if 'dataset_name' in case else ""
            print(f"\nğŸ”¸ æ¡ˆä¾‹ {case['case_id']}{dataset_info}: {case['case_name']}")
            print(f"   è¾“å…¥: {case['input'][:100]}...")
            print(f"   æ•´ä½“æˆåŠŸ: {case['success']}")
            
            # æ˜¾ç¤ºå„ä¸ªæŒ‡æ ‡
            for key, value in case.items():
                if key.endswith('_score'):
                    metric_name = key[:-6]
                    score = value
                    success = case.get(f'{metric_name}_success', False)
                    reason = case.get(f'{metric_name}_reason', '')[:100]
                    print(f"   ğŸ“Š {metric_name}: åˆ†æ•°={score}, æˆåŠŸ={success}")
                    if reason:
                        print(f"      åŸå› : {reason}...")
                    # å±•ç¤ºè¯¥æŒ‡æ ‡çš„æ‹†è§£ç»´åº¦åˆ†æ•°
                    breakdown_items = [(bk, bv) for bk, bv in case.items() if bk.startswith(f'{metric_name}_breakdown_')]
                    if breakdown_items:
                        print(f"      æ‹†è§£ç»´åº¦åˆ†æ•°:")
                        for bk, bv in sorted(breakdown_items):
                            dim = bk.replace(f'{metric_name}_breakdown_', '')
                            print(f"        - {dim}: {bv}")
            # å±•ç¤ºé¢„æœŸå·¥å…·è°ƒç”¨ï¼ˆæ¯ä¸ªç”¨ä¾‹ï¼‰
            if case.get('expected_tool_calls'):
                preview = case['expected_tool_calls'][:200] + ("..." if len(case['expected_tool_calls']) > 200 else "")
                print(f"   ğŸ”§ Expected tool calls: {preview}")
    
    # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆç‹¬ç«‹çš„è¾“å‡ºæ–‡ä»¶å
    task_output_prefix = f"{output_prefix}/{task_name}"
    
    # ä¿å­˜ç»“æœ
    save_to_formats(all_task_cases, summary, task_output_prefix, multi_dataset=multi_dataset)
    
    print(f"\nâœ… ä»»åŠ¡ {task_name} åˆ†æå®Œæˆ! å…±å¤„ç† {total_cases} ä¸ªæµ‹è¯•æ¡ˆä¾‹")

def main():
    parser = argparse.ArgumentParser(description='æå–æµ‹è¯•æ¡ˆä¾‹çš„metricsæ•°æ®')
    parser.add_argument('--input', '-i', 
                       nargs='+',  # æ”¯æŒå¤šä¸ªè¾“å…¥æ–‡ä»¶
                       default=['.deepeval/.latest_test_run.json'],
                       help='è¾“å…¥çš„æµ‹è¯•è¿è¡ŒJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯æŒ‡å®šå¤šä¸ªï¼‰')
    parser.add_argument('--output', '-o',
                       default='metrics_analysis',
                       help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    parser.add_argument('--show-details', '-d',
                       action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†çš„æ¡ˆä¾‹ä¿¡æ¯')
    parser.add_argument('--dataset-names', '-n',
                       nargs='+',
                       help='ä¸ºæ¯ä¸ªè¾“å…¥æ–‡ä»¶æŒ‡å®šdatasetåç§°ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--per-task', '-t',
                       action='store_true',
                       help='ä¸ºæ¯ä¸ªtaskç”Ÿæˆç‹¬ç«‹çš„æŠ¥å‘Šæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_files = args.input
    dataset_names = args.dataset_names or []
    
    # ç¡®ä¿dataset_namesæ•°é‡ä¸è¾“å…¥æ–‡ä»¶æ•°é‡åŒ¹é…
    if dataset_names and len(dataset_names) != len(input_files):
        print(f"âŒ é”™è¯¯: dataset_namesæ•°é‡({len(dataset_names)})ä¸è¾“å…¥æ–‡ä»¶æ•°é‡({len(input_files)})ä¸åŒ¹é…")
        return
    
    # æ£€æŸ¥æ‰€æœ‰è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for input_file in input_files:
        if not Path(input_file).exists():
            print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return
    
    print(f"ğŸ” æ­£åœ¨åˆ†æ {len(input_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
    for i, input_file in enumerate(input_files):
        dataset_name = dataset_names[i] if i < len(dataset_names) else None
        print(f"  - {input_file}" + (f" (dataset: {dataset_name})" if dataset_name else ""))
    
    # æå–æ‰€æœ‰æ•°æ®
    try:
        all_extracted_data = []
        
        # å¤„ç†æ¯ä¸ªè¾“å…¥æ–‡ä»¶
        for i, input_file in enumerate(input_files):
            print(f"\nğŸ“Š å¤„ç†æ–‡ä»¶ {i+1}/{len(input_files)}: {input_file}")
            
            # æå–è¯¥æ–‡ä»¶çš„æ•°æ®
            task_data_dict = extract_metrics_data(input_file)
            
            if args.per_task:
                # ä¸ºæ¯ä¸ªtaskç”Ÿæˆç‹¬ç«‹æŠ¥å‘Š
                for task_name, task_data in task_data_dict.items():
                    process_single_task(task_name, task_data, args.output, args.show_details)
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼šåˆå¹¶æ‰€æœ‰æ•°æ®
                for task_name, task_data in task_data_dict.items():
                    for dataset_name, cases in task_data.items():
                        all_extracted_data.extend(cases)
                
                print(f"   æå–äº† {len(all_extracted_data)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
        
        if not args.per_task:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šç”Ÿæˆæ€»ä½“æŠ¥å‘Š
            if all_extracted_data:
                # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¤šdatasetæ¨¡å¼
                multi_dataset = any('dataset_name' in case for case in all_extracted_data)
                
                # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡
                summary = create_metrics_summary(all_extracted_data, group_by_dataset=multi_dataset)
                
                # æ‰“å°æ¦‚è§ˆ
                print_metrics_overview(summary, multi_dataset=multi_dataset)
                
                # ä¿å­˜ç»“æœ
                save_to_formats(all_extracted_data, summary, args.output, multi_dataset=multi_dataset)
                
                print(f"\nâœ… åˆ†æå®Œæˆ! å…±å¤„ç† {len(all_extracted_data)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
            else:
                print("âš ï¸ æ²¡æœ‰æå–åˆ°ä»»ä½•æµ‹è¯•æ¡ˆä¾‹")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
